Probabilistic Machine Learning: An Introduction
https://probml.gthub.io/pml-book/book1.html
Pattern Recognition and Machine Learning

*Learning* from data
*Prediction* new data

![](../img/Pasted%20image%2020250918203215.png)
model `f`
input `x` : features or oberservations or measurements about the real-parameters $\color{#b293f6}\theta$: things going to learn from data
output `y`

*Manually defining exhaustivev rules for a given task can be very challenging, as it is difficult to account for all possibilities.*

predefined rules
measure a given real y-value and prediction from model
![](../img/Pasted%20image%2020250918204821.png)
`l()` is a function that measures if the model's predictions are correct

choosing parameter
similar data as before

- What task am I trying to solve
- How should I **model the problem**
- How should I **represent** my data
- How can I **estimate the parameters** of my model
- How should I measure the **performance** of my model

# Supervised Learning
goal is a function `f`, inputs `x` and output `y`
x and y are all given in the training data
![](../img/Pasted%20image%2020250918211650.png)
predicting some continuous quantity
## Classification
given an input feature vector x, predicting an output vector y

- binary classivication: two possibilities
- bulticlass classification: C possible options

### Generative Classification
é€šè¿‡é«˜æ–¯åˆ†å¸ƒç¡®å®šç‰¹å¾
- Assumption: conditined on the class, Data is Gaussian distributed\
![](../img/Pasted%20image%2020250918222504.png)

How likely it is that a test datapoint is from a given class
- evaluate the likelihood and divide by tatal
*Prior Knowledge*: Weighting factor
![](../img/Pasted%20image%2020250918224134.png)

*Bayes Classifier*
![](../img/Pasted%20image%2020250918224309.png)
Make prediction about new data that never seen before
![](../img/Pasted%20image%2020250918225018.png)

è¯æ®å’Œç»“æœçš„åŸºäºæ¦‚ç‡çš„å†³ç­–å…³ç³»
ä¸ç¡®å®šæ€§
è¯æ® --> è®¤çŸ¥
model fitting / model training
#### Maximum Likelihood Estimation MLE
*Likelihood function*
![](../img/Pasted%20image%2020250919190108.png)
*iid assumption*: independent and identically sampled from the same distribution

*Log Likelihood* : preserve the ordering
![](../img/Pasted%20image%2020250919190524.png)

*Negative Log Likelihood*
æœ€å°åŒ–æŸå¤±å‡½æ•°
![](../img/Pasted%20image%2020250919191947.png)
- å¯¹æ ‡ç­¾çš„NLL, $\color{#b293f6}\theta_b$â€‹ï¼šæ§åˆ¶æ ‡ç­¾åˆ†å¸ƒï¼ˆå…ˆéªŒï¼ŒBernoulli åˆ†å¸ƒï¼‰
- å¯¹ç‰¹å¾çš„NLL, $\color{#b293f6}\theta_g$â€‹ï¼šæ§åˆ¶ç‰¹å¾åˆ†å¸ƒï¼ˆæ¡ä»¶åˆ†å¸ƒï¼ŒGaussian åˆ†å¸ƒï¼‰

#### Mernoulli Distribution
![](../img/Pasted%20image%2020250919205721.png)
NLL
![](../img/Pasted%20image%2020250919225000.png)
MLE
![](../img/Pasted%20image%2020250919230310.png)

#### Gaussian Likelihood
![](../img/Pasted%20image%2020250920001300.png)

![](../img/Pasted%20image%2020250920004457.png)

![](../img/Pasted%20image%2020250920011930.png)

#### Multivariate Classification
Define model for multivariate data å¤šå˜é‡

Mean vector
![](../img/Pasted%20image%2020250920012818.png)

##### Convariance matrix
![](../img/Pasted%20image%2020250920012855.png)
The main diagnal contains *variances*, i.e. the *covariance* of each dimension with itself, for a given random vector $\color{#bd93f9}x=[x1â€‹,x2â€‹]^âŠ¤$
![](../img/Pasted%20image%2020250923193913.png)
- symmetric
- Generalises the notion of variance to **multiple dimensions**
- positive semi-definite xâŠº ğšºx â‰¥ 0 and xâŠº ğšºâˆ’1x â‰¥ 0
- full convariance matric has D(D+1)/2 free parameters

![](../img/Pasted%20image%2020250923234808.png)

- **LDA (Linear Discriminant Analysis)**ï¼šå‡è®¾æ‰€æœ‰ç±»åˆ«å…±äº«åŒä¸€ä¸ªåæ–¹å·®çŸ©é˜µ Î£\SigmaÎ£ã€‚ â†’ å†³ç­–è¾¹ç•Œæ˜¯çº¿æ€§çš„ã€‚
- **QDA (Quadratic Discriminant Analysis)**ï¼šæ¯ä¸ªç±»åˆ«éƒ½æœ‰è‡ªå·±çš„åæ–¹å·®çŸ©é˜µ Î£c\Sigma_cÎ£câ€‹ã€‚ â†’ å†³ç­–è¾¹ç•Œæ˜¯äºŒæ¬¡æ›²çº¿ã€‚

![](../img/Pasted%20image%2020250923194233.png)

y âˆˆ {1, ..., C} and C > 2, by simply defining a class conditional model p(x|y = c) for each class

#### Naive Bayes
*Probabilisitc model* for conditional density $\color{#bd93f9}p(x|y)$, assume that **features are conditionally independent given the class label**
![](../img/Pasted%20image%2020251002165008.png)

*independent*: one variable does not affect another, `A` is (marginally) independent of `B` if $$p(A|B)=P(A)$$, which in conditional probability, $$p(A, B)=P(A)P(B)$$

`A` is *Conditionally independent* of `C` given `B` if $$p(A|C, B)=p(A|B)$$ i.e. once we know `B`, knowing `C` does not provide additional information about `A`

![](../img/Pasted%20image%2020251002180731.png)

![](../img/Pasted%20image%2020251002184116.png)
![](../img/Pasted%20image%2020251002184812.png)
![](../img/Pasted%20image%2020251002184846.png)

![](../img/Pasted%20image%2020251002184920.png)

*Laplace smoothing*: add a small positive number to all counts
![](../img/Pasted%20image%2020251002192253.png)

Missing Data: simply **ignore** the feature in any instance where the value is missing

##### Continuous Data
![](../img/Pasted%20image%2020251002203405.png)

![](../img/Pasted%20image%2020251002203904.png)

The conditional independence assumption used by Naive Bayes can fail to capture relationships that may be present in some datasets

### Discriminative Classifiers
Unlike Generative classifiers, the *discriminative classifiers* do not modelling the generative process

Discriminative approaches directly model the posterior $\color{#bd93f9}p(y|x)$
![](../img/Pasted%20image%2020251002232234.png)

#### Binary Linear Classification
Given some input features `x`, with associated class labels `y`, the goal is to estimate the parameters `w` of a *hyperplane* that can separate the data into the two classes

The *decision boundary* is where teh two classes are tied
- In 2D, **line**
- In 3D, **plane**
- In higher dimensions, **hyperplane**

![](../img/Pasted%20image%2020251002232909.png)

Decision boundary: $\color{#bd93f9}w^âŠ¤Ï•(x)=0$
![](../img/Pasted%20image%2020251002235103.png)

If we can find a hyperplane to separate the data based on the class labels, the problem is said to be *linearly separable*

#### Logistic Regression
Model predictionos need be in range `[0, 1]`

*Logistic function*

![](../img/Pasted%20image%2020251003000434.png)
As `z` goes from $\color{#bd93f9}-\infty$ to $\color{#bd93f9}\infty$, $\color{#bd93f9}\sigma(z)$ goes from 0 to 1, has a **sigmoid** shape

![](../img/Pasted%20image%2020251003000916.png)
Modifying the input to the logistic function changes the shape of the function

Logistic regression = **linear weights + logistic squashing function**
![](../img/Pasted%20image%2020251003001443.png)

Has a linear decision boundary $\color{#bd93f9}p(y=1|x;w)=p(y=0|x)=0.5$, which equivalent to $\color{#bd93f9}w^âŠ¤Ï•(x)=0$
![](../img/Pasted%20image%2020251003002549.png)

![](../img/Pasted%20image%2020251004014446.png)

- `w0` bias term
- `w1` weight for feature 1
	- å¦‚æœ $w_1 > 0$ï¼Œè¯´æ˜ $x_1$â€‹ è¶Šå¤§ï¼Œè¶Šå€¾å‘äºé¢„æµ‹ä¸ºæ­£ç±»ï¼ˆy=1ï¼‰ã€‚
	- å¦‚æœ $w_1 < 0$ï¼Œè¯´æ˜ $x_1â€‹$ è¶Šå¤§ï¼Œè¶Šå€¾å‘äºé¢„æµ‹ä¸ºè´Ÿç±»ï¼ˆy=0ï¼‰ã€‚
- `w2` weight for feature 2

![](../img/Pasted%20image%2020251004234038.png)
![](../img/Pasted%20image%2020251004234109.png)
åœ¨å‚æ•° `w` ä¸‹ï¼Œæ‰€æœ‰æ ·æœ¬çš„æ ‡ç­¾åˆšå¥½ç­‰äºè§‚æµ‹åˆ°çš„ `y` çš„æ¦‚ç‡
æœ€å¤§ä¼¼ç„¶ä¼°è®¡MLEå°±æ˜¯ **æ‰¾åˆ°æœ€ä¼˜çš„ `w`ï¼Œè®©è§‚æµ‹æ•°æ®çš„æ¦‚ç‡æœ€å¤§**
![](../img/Pasted%20image%2020251004234344.png)
$\color{#bd93f9}x_{nd}$â€‹ï¼šå°†è¯¯å·®æŒ‰æ¯ä¸ªç‰¹å¾ç»´åº¦çš„è´¡çŒ®è¿›è¡ŒåŠ æƒ

*One-vs-Rest* classification
- train an separate classifier, with an associated weight vector $\color{#bd93f9}w_c$, for each class
![](../img/Pasted%20image%2020251004235857.png)
- We select the maximum of the different classifiers as the predicted class
![](../img/Pasted%20image%2020251005000556.png)


*Softmax* function
![](../img/Pasted%20image%2020251005002811.png)
where![](../img/Pasted%20image%2020251005002832.png)![](../img/Pasted%20image%2020251005002927.png)
#### Linear Regression
The relationship between the features `x` and the target `y` is linear

![](../img/Pasted%20image%2020251005192519.png)
![](../img/Pasted%20image%2020251005192440.png)

The solved weights tell us the contribution of each feature to the final prediction
- Weight that close to 0 --> feature does not influence the output
- Large positive value --> strong positive relationship
- Large negative value --> string negative relationship

Standardise the data: interpret the relative model weights across the different dimensions, for each feature dimension from the training set:
- Mean
- Standard deviation

![](../img/Pasted%20image%2020251005231305.png)
*Sum of Squared Errors (SSE)*
![](../img/Pasted%20image%2020251005231359.png)
![](../img/Pasted%20image%2020251005235144.png)

![](../img/Pasted%20image%2020251006000538.png)

![](../img/Pasted%20image%2020251006001329.png)

Sensitive to *outliers*:
- Is the relationship obviously nonlinear
- Are there obvious outliers?

#### Non-linear Regression
*Polynomial regression*, the dimensionality of weights `w` will be the same as $\color{#bd93f9}ğœ™(x)$
![](../img/Pasted%20image%2020251006005057.png)

##### Basis Expansion
Transform the original features `x` non-linearly into $\color{#bd93f9}ğœ™(x)$ and *perform linear regression* on the transformed features, like a set of `M` basis functions
![](../img/Pasted%20image%2020251006005109.png)

So, the model becomes
![](../img/Pasted%20image%2020251006005133.png)

##### Radial Basis Functions (RBFs)
Each RBF $\color{#bd93f9}ğœ“ ()_m$ has two parameters, a bell-shaped curve
- A centre location $\color{#bd93f9}c_m$
- A width $\color{#bd93f9}ğœ^2_m$
- outputs a single scalar
![](../img/Pasted%20image%2020251006005507.png)
Choosing a subset of the datapoints as centres

#### Decision Trees
Nonlinear Data: classification and regression
Nodes:
- Root
- Internal
- Leaves

At each node, split data based on *feature dimension* and *threshold ğœƒ*

1. All the data at the root node of the tree
2. Grow the tree by recursively splitting the data at each node
3. Until reach a specified condition
	- predefined maximum depth
	- impossible to split the data further

**Favour splits that result in child nodes that have high 'purity' i.e. 'impurity' `I(S)`** 

Measure the *entropy* at each node, using the distribution of datapoints
![](../img/Pasted%20image%2020251007224521.png)
Also notated as `H(S)`

*Gini Impurity*
![](../img/Pasted%20image%2020251007224807.png)

Measure the *Information Gain* of a split for each feature dimension and threshold pair, choose the **largest gain**
![](../img/Pasted%20image%2020251007225533.png)

"Singleton nodes" will be 100% purity ==> **overfitting**
Additional *hyperparameter* to prune it
- Maximum tree depth
- Minimum number of datapoints per node
- Minimum information gain

##### Regression trees
Ground truth targets are continuous values, *Node purity*: variance
![](../img/Pasted%20image%2020251007232448.png)
At each leaf we store the **mean** of all the datapoints that arrived at the node
![](../img/Pasted%20image%2020251007232534.png)

##### Ensembles of Trees
> è®­ç»ƒå¤šæ£µç¨æœ‰å·®å¼‚çš„å†³ç­–æ ‘ï¼Œç„¶å**è®©å®ƒä»¬å…±åŒæŠ•ç¥¨æˆ–å¹³å‡é¢„æµ‹ç»“æœ**ã€‚
> è¿™æ ·å•ä¸ªæ ‘çš„â€œå™ªå£°â€ä¼šè¢«å¹³å‡æ‰ï¼Œæ•´ä½“æ¨¡å‹æ›´ç¨³ã€æ›´å‡†ç¡®ã€‚

Grow an ensemble of `K` different decision trees
1. Pick a random subset of the data
2. Train a decision tree on this data, when splitting, choose a random subset of features
3. Repeat this `K` different times

Given a new datapoint `x` at test time: **classify `x` separately using each tree**
- Classification: majority vote
- Regression: mean prediction

## Optimisation
"Learning" --> continuous optimisation
descending error surface, minimise *error function* $\color{#bd93f9}\mathcal{L}(w)$
When data is i.i.d. (Independent and Identically Distributed)
![](../img/Pasted%20image%2020251018175302.png)

*Smoothness*
Constrained/continuous data, $\color{#bd93f9}\mathcal{L}(w)$ provides information about $\mathcal{L}$ and nearby values

*Derivatives*
For differentiable $\color{#bd93f9}\mathcal{L}(w)$, partial derivatives $\frac{\partial \mathcal{L}}{\partial w_i}$
vector of partial derivatives = gradient of the error: steepest error ascent è¯¯å·®ä¸Šå‡æœ€å¿«çš„æ–¹å‘ $$\nabla_w \mathcal{L} = \left( \frac{\partial \mathcal{L}}{\partial w_1}, \frac{\partial \mathcal{L}}{\partial w_2}, \dots, \frac{\partial \mathcal{L}}{\partial w_N} \right)$$
for descent æœ€å°åŒ–è¯¯å·®: $$-\nabla_w \mathcal{L}$$

*Optimisation algorithm* $$\underset{w}{min}\space\mathcal{L}(w)$$
![](../img/Pasted%20image%2020251018212110.png)
> ä¸æ–­è®¡ç®—æ¢¯åº¦ â†’ ç¡®å®šä¸‹é™æ–¹å‘ â†’ æ²¿è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°å‚æ•° â†’ ç›´åˆ°è¯¯å·®è¶³å¤Ÿå°ä¸ºæ­¢

$\color{#bd93f9}\eta$: step size

![](../img/Pasted%20image%2020251019002906.png)
> â€œTaking a step along Î´ cannot increase value locallyâ€
> åªè¦æ­¥é•¿è¶³å¤Ÿå°ï¼Œæ²¿ç€è´Ÿæ¢¯åº¦æ–¹å‘ï¼ˆÎ´æ–¹å‘ï¼‰å‰è¿›ï¼Œä¸ä¼šè®©æŸå¤±ä¸Šå‡ã€‚

![](../img/Pasted%20image%2020251019010019.png)

### Gradient Descent
`compute directiono d=g`
![](../img/Pasted%20image%2020251019010231.png)

Computation
![](../img/Pasted%20image%2020251019013925.png)
- Estimation requires evaluating gradients at all N data points

*Stochastic Gradient Descent*
Compute update for parameter with just a single instance
![](../img/Pasted%20image%2020251019014754.png)
- Choose randomly for $\color{#bd93f9}i\in{1,...,N}$
- $\mathrm{E}[\nabla_w\mathcal{L}^i(w)] = \nabla_w\mathcal{L}(w)$
Provides an unbiased estimate of the gradient at each step

Struggle close to optimum

*Mini-batches Stochastic Gradient Descent*
![](../img/Pasted%20image%2020251019021044.png)
reduces the variance of the gradient estimator by `1/B`, also `B` times more expensive to compute `O(B * D)`

## Generalisation
Overfitting & Underfitting

*Qualitative*: Traning Data, Model Parameters
*Goldkocks Zone*: sufficient capacity to lean true regularities, but not enough to memorise or expoit accedential regularities

Control capacity: **model hyper-parameters**, to minisise generalisation error
- Regression: polynominal order
- Naive Bayes: # attributes, bounds on $\color{#b293f6}/sigma^2$
- Decision Trees # nodes

### Measuring Generalisation
Generalise to **novel** and **unseed** instances
Estimate error on test data without training on test data

#### Cross Validation
Partition data into train/test in different ways $$\{\mathcal{D}_{train_1};\mathcal{D}_{test_1}\},...,\{\mathcal{D}_{train_K};\mathcal{D}_{test_K}\}$$
For each partition: train model on training data -> test error on test data
**Find model from partition with lowest test error**
**Typically for small data**

#### Train-Val-Test
Hyper-parameters $$\mathcal{D}=\{\mathcal{D}_{train};\mathcal{D}_{val};\mathcal{D}_{test}\}$$
Tuning typer-parameters on $\mathcal{D}_{val}$
- for every candidate set of hyper-parameters, train on $\mathcal{D}_{train}$
- evaluate error on $\mathcal{D}_{val}$
- Find the hyper-parameters that lowest error on $\mathcal{D}_{val}$
Test error on $\mathcal{D}_{test}$
**Typically for big data that hard to 'cross validate'**

*Modelling Generalisation Error*
![](../img/Pasted%20image%2020251020214149.png)

#### Bias and Variance
![](../img/Pasted%20image%2020251020215742.png)
Bayes Error: å³ä½¿çŸ¥é“xä¹Ÿæ— æ³•æ¶ˆé™¤çš„ç›®æ ‡éšæœºæ€§

![](../img/Pasted%20image%2020251020220044.png)
> æ¨¡å‹é¢„æµ‹çš„**å¹³å‡å€¼**ä¸çœŸå®å‡å€¼å·®è·ï¼ˆ$423K vs $420Kï¼‰å°±æ˜¯ **biasï¼ˆåå·®ï¼‰**ï¼›
> ä¸åŒè®­ç»ƒå¾—åˆ°çš„é¢„æµ‹å€¼ä¹‹é—´çš„æ³¢åŠ¨æ˜¯ **varianceï¼ˆæ–¹å·®ï¼‰**ï¼›
> åŒæ ·æˆ¿å­çš„çœŸå®å”®ä»·è‡ªèº«çš„æ³¢åŠ¨ï¼ˆ$410K~$430Kï¼‰å°±æ˜¯ **Bayes errorï¼ˆå™ªå£°ï¼‰**ã€‚

![](../img/Pasted%20image%2020251020223339.png)

### Improving Generalisation
Reducting overfitting
- Reducing capacity: tune on a validation set
- Early stopping: stop training when generalisation error starts to increase
- Ensembles: train different models on random subsets of training data and **averaging predictions** from multiple models reduces variance
- Regularisation

#### Regularisation
Penalise parameters that may be pathological and unlikely to generalise well
Complexity cost
![](../img/Pasted%20image%2020251020223543.png)

Linear regression    Ridge
![](../img/Pasted%20image%2020251020224049.png)

![](../img/Pasted%20image%2020251020230903.png)


# Data Exploration and Evaluation
## Representing Data

*Core Questions*
- What task am I trying to solve?  
- How should I model the problem?  
- How should I represent my data?  
- How can I estimate the parameters of my model?  
- How should I measure the performance of my model?

![](../img/Pasted%20image%2020251011213935.png)
Present data mathematically (feature representation)

**Feature-Value Pairs**

*Categorical Features*
- Each instance falls into one of a set
- Mutually exclusive
- Typically encoded as numbers that index into the set
- No natural 'ordering', 'closeness', only equality

*Ordinal Features*
- Each instance falls into one of a set
- Natural ordering to the categories (increasing/decreasing)
- Compare (<, >, =) only

*Numeric Features*
- Integers or real numbers
- comparison, closeness, functions(mean, variance)
- Extendable to higher order features

![](../img/Pasted%20image%2020251012015954.png)

### Modern representation
Learn what values for features helps with doing the task well
> å­¦ä¹ è·å¾—æœ‰æ•ˆç‰¹å¾è€Œä¸æ˜¯æ‰‹å·¥è®¾è®¡ç‰¹å¾

Choose a basic set of attributes, say image â€œpatchesâ€
![](../img/Pasted%20image%2020251012020205.png)

> Consider both data and task
> Consider what kind of model you want
> 	nuanced, interpretable
> 	scalable, performant
> Opaque decision making over learnt taskâ€‘specific features

### Plotting Data
Features
- title
- labelled axes
- axes ranges and ticks
- clarity (colour / thickness)
- legend

Informative: Convey as much as necessary
Clean: avoid overfilling & redundancy

#### Dimensionality Reduction
> *Manifold Hypothesis*: High-dimensional data in the real world really lies on low-dimensional manifolds within that high-dimensional space

**High dimensionality**
- **Counting** problem: As dimensionality grows, fewer abservations per region
- Mitigation
	- domain knowledge
	- feature engineering
	- modelling assumptions about features independence smoothness symmetry
	- reduce data dimensionality, struct a new set

*Dimensionality Reduction*
Represent data using a "few" variables
- *compression*: preserve as much information / structure as possible
- *discrimination*: only keep information that enables task
	- classification

*Feature selection*
- subset all features
- relevant to task

*Featrue Transformation*
- construct a new set of dimensions
![](../img/Pasted%20image%2020251013194650.png)
- transformation of original: linear F =â‡’ e = Fx

##### Principal Components Analysis
Define printcipal commponents
- 1st PC: direction of **greatest** variation in the data
- 2nd PC: âŠ¥ 1st PC; **greatest remaining** variation
- ... until D, for $\color{#b293f6}x\in\mathrm{R}^D$

First $M << D$ components become new basis dimensions
Transform coordinates of each data point to new basis

Variation along direction = information
Transform basis --> fit maximum informaiton into M dimensions

![](../img/Pasted%20image%2020251013210933.png)
**ä¸æ–­ä¹˜ä»¥ S ä¼šæŠŠä»»æ„éé›¶å‘é‡çš„æ–¹å‘æ¨å‘ S çš„*ä¸»ç‰¹å¾å‘é‡***ï¼ˆå¯¹åº”æœ€å¤§ç‰¹å¾å€¼çš„é‚£ä¸ªå‘é‡)

![](../img/Pasted%20image%2020251013214241.png)
ä»»æ„å•ä½å‘é‡ $v$ ä¸Šçš„æŠ•å½±æ–¹å·®æ˜¯ $v^TSv$, è€Œ PCA æ­£æ˜¯å¯»æ‰¾è®©è¿™ä¸ªæ–¹å·®æœ€å¤§çš„æ–¹å‘$v$
- æ–¹å‘ $v$ æ˜¯ $S$ çš„ç‰¹å¾å‘é‡;
- æ–¹å·®å¤§å°ï¼ˆæŠ•å½±åä¿¡æ¯é‡ï¼‰= å¯¹åº”çš„ç‰¹å¾å€¼ $\lambda$ã€‚

![](../img/Pasted%20image%2020251013221556.png)

![](../img/Pasted%20image%2020251013222654.png)

*Explained Variance*: Find minimum $M$, such![](../img/Pasted%20image%2020251013222810.png)
ç´¯è®¡æ–¹å·®æ¯”ä¾‹ / ç‰¹å¾å€¼ä¸‹é™è¶‹åŠ¿

![](../img/Pasted%20image%2020251013223835.png)
é™ç»´ï¼šâ€œæŠŠé«˜ç»´æ•°æ®åœ¨ä¸»æˆåˆ†æ–¹å‘ä¸ŠæŠ•å½±â€ã€‚
åŸæ¥çš„ D ç»´å‘é‡ç°åœ¨ç”¨ M ä¸ªä¸»æˆåˆ†åæ ‡ $e_i$â€‹ æ¥è¡¨ç¤ºã€‚

é‡æ„ï¼šâ€œä»ä½ç»´ç©ºé—´è¿‘ä¼¼åœ°æ¢å¤åŸå§‹æ•°æ®â€
M è¶Šå¤§ï¼Œé‡æ„è¶Šæ¥è¿‘åŸæ•°æ®

##### Limitations
Sensitivity: **outliers** changes variance and pricipal components
- normalise: zero mean unit variance![](../img/Pasted%20image%2020251013233209.png)
- find outliers using interquartile range (IQR)
	- median(upper quartile 25%) â€‘ median(lower quartile 75%)
	- define 'outlier' as values > 1.5\*IQR

Transform to handle non-linearity
LDA supervised

# Unsupervised Learning
have no label values
![](../img/Pasted%20image%2020250918211823.png)

# Reinforcement Learning
Learn how to interact with its environment
Learn a **policy** which specifies the action to take in response to observation of the environment
Agents aims to maximise their reward

# Ethics

